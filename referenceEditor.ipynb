{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18ecca1",
   "metadata": {},
   "source": [
    "# GLANSIS REFERENCE CLEANER & BULK UPLOADER\n",
    "**Description:** The following scripts will help you to better clean and bulk upload references\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b86e3f",
   "metadata": {},
   "source": [
    "**Installing Libraries:** To run this script, there are several necessary packages that need to be installed. Below is the quick and easy way to install the necessary pacakges to run this code. You only need to run it the first time you run this script. After that, the packages will be installed in your system. For that reason, I have the code commented out because there should be no reason to run any other time. IF this is your first time, remove the '#' to uncomment the second line ('pip install requirement.text'). Make sure the requirements.txt is in the main folder. \n",
    "\n",
    "*Be Aware: This is not the 'proper' way to initalize a script. If you find yourself running multiple scripts for different project and are frequently installing new packages, you should create a virtual environment. There is plenty of resources online explaining how to so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Libaries - keep this commented after firest use\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc860f",
   "metadata": {},
   "source": [
    "# PART 1: REFERENCE CLEANER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d1dad",
   "metadata": {},
   "source": [
    "## 1. Exporting from EndNote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf9c18",
   "metadata": {},
   "source": [
    "To correctly get your Journal Article references out of Endnote into a text file that Jupyter Notebooks can read, following the following steps:\n",
    "1. Download and save the GLANSIS_refManagerExport output style to the EndNote>Styles folder.\n",
    "2. If first time, go task bar and select the Biolographic Output Style dropdown bar. Click Select Another Style. Scroll to GLANSIS_refManagerExport. Click on GLANSIS_refManagerExport. CLick 'Choose.'\n",
    "2. Select all the journal articles you want to enter into NAS\n",
    "3. Click File> Export. In the pop-up make to change preferences to:<br>\n",
    "    a. File name: Enter your file name of choice<br>\n",
    "    b. Save as type: Text File (.txt)<br>\n",
    "    c. Output style: GLANSIS_csv_ref_export<br>\n",
    "    d. Click Save<br>\n",
    "4. To make it easier for the code to find PDF, extract all PDFs from EndNote library and put into new folder.<br>\n",
    "    a. Go to directory where the EndNote Library is stored. Click on the .Data folder.<br>\n",
    "    b. There will be a PDF and sdb folder inside. Click on the PDF folder.<br>\n",
    "    c. In the search bar type '.pdf'. Click on the first PDF and hit Ctrl + A.<br>\n",
    "    d. Copy and paste PDFs into a new folder. I recommend in the same folder as your EndNote Library outside your .Data file<br>\n",
    "5. Once this is done, you should be ready to start editing your\n",
    "    \n",
    "\n",
    "*This will only work for journal articles which is the bulk of what we handle. Any reports, websites, or other references will need to be entered by hand. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd117aaf",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da4ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\redinger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "import os                        # Mananages directories\n",
    "import numpy as np               # Used to manage NA values\n",
    "import pandas as pd              # Manages dataframes\n",
    "import re                        # Edits text strings\n",
    "from tkinter import filedialog   # Creates file dialog box\n",
    "import requests                  # Pulls HTML code from webpage\n",
    "from bs4 import BeautifulSoup    # HTML parsing\n",
    "import fitz                      # Open and pdf manipulation - package for PyMuPDF\n",
    "import unicodedata               # Convert extracted keywords to unicode - removes issues with duplication\n",
    "import nltk                      # Library of natural language processing tools              \n",
    "import nltk.corpus               # Access corpora\n",
    "import string                    # Format text strings\n",
    "import pickle                    # Serailizes and deserializing models\n",
    "import math                      # Math functions for numerical computations\n",
    "import nltk.data                 # Retrieves data files and resources\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer   # Concatenates token sequences\n",
    "from nltk.tokenize import word_tokenize   # Splits words into tokens\n",
    "nltk.download('punkt')                    # Pre-trained tokenizer\n",
    "from openpyxl import load_workbook        # Edit Excel sheets\n",
    "from openpyxl.styles import PatternFill   # Modify Excel formatting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea28e1",
   "metadata": {},
   "source": [
    "## 3. Set Up for Cleaning:\n",
    "Add the relavent scientific and common names for your current species. These will be used for the creation of key words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a80ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include any relevant scientific name and common name\n",
    "species_names = ['scientific name', 'common name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a05f2",
   "metadata": {},
   "source": [
    "By running the cell below, you will open a file dialog box with your computers directory. You may have to minimize the current window to see it (sometimes it likes to hide behind other open windows). Then travel to the folder containing the excel sheet with your references that need cleaned. Select the file and hit 'open.' The first five rows of reference sheet will appear below if you have uploaded correctly. Check to make sure everything looks correct.\n",
    "\n",
    "* Note: EndNote sometimes has trouble with formatting references from older PDFs. If you find yourself getting an error code like this: ''. Try importing the .txt file into Excel as a comma delimited file to find the problem reference. Those references unfortunantly will need to be entered by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae9707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Journal Name</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Pages</th>\n",
       "      <th>URL</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>DOI</th>\n",
       "      <th>PDF Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Covich, A. P., L. L. Dye, and J. S. Mattice</td>\n",
       "      <td>1981</td>\n",
       "      <td>Crayfish predation on corbicula under laborato...</td>\n",
       "      <td>American Midland Naturalist</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>181-188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crayfish predation on the Asiatic clam Corbicu...</td>\n",
       "      <td>10.2307/2425023</td>\n",
       "      <td>internal-pdf://1550862459/Covich-1981-Crayfish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Creed, R. P., and J. M. Reed</td>\n",
       "      <td>2004</td>\n",
       "      <td>Ecosystem engineering by crayfish in a headwat...</td>\n",
       "      <td>Journal of the North American Benthological So...</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>224-236</td>\n",
       "      <td>http://www.journals.uchicago.edu/doi/citedby/1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AbstractWe conducted an enclosure/exclosure ex...</td>\n",
       "      <td>10.1899/0887-3593(2004)023&lt;0224:eebcia&gt;2.0.co;2</td>\n",
       "      <td>internal-pdf://1645103618/Creed-2004-Ecosystem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Distefano, R. J., R. J. Neves, L. A. Helfrich,...</td>\n",
       "      <td>1991</td>\n",
       "      <td>Response of the crayfish cambarus bartonii bar...</td>\n",
       "      <td>Canadian Journal of Zoology</td>\n",
       "      <td>69</td>\n",
       "      <td>6</td>\n",
       "      <td>1585-1591</td>\n",
       "      <td>https://app.dimensions.ai/details/publication/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Intermolt adult and juvenile Cambarus bartonii...</td>\n",
       "      <td>10.1139/z91-222</td>\n",
       "      <td>internal-pdf://0683493655/Distefano-1991-Respo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Fauth, J. E.</td>\n",
       "      <td>1990</td>\n",
       "      <td>Interactive effects of predators and early lar...</td>\n",
       "      <td>Ecology</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>1609-1616</td>\n",
       "      <td>https://app.dimensions.ai/details/publication/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ecologists need to determine the frequency and...</td>\n",
       "      <td>10.2307/1938296</td>\n",
       "      <td>internal-pdf://4101458137/Fauth-1990-Interacti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Gelder, S. R., L. McCurry, and D. F. McAlpine</td>\n",
       "      <td>2009</td>\n",
       "      <td>Distribution and first records of branchiobdel...</td>\n",
       "      <td>Northeastern Naturalist</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>45-52</td>\n",
       "      <td>https://bioone.org/journals/northeastern-natur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Preserved specimens of the crayfish Cambarus b...</td>\n",
       "      <td>10.1656/045.016.0104</td>\n",
       "      <td>internal-pdf://2457356880/Gelder-2009-Distribu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Type                                             Author  Year  \\\n",
       "0  Journal Article        Covich, A. P., L. L. Dye, and J. S. Mattice  1981   \n",
       "1  Journal Article                       Creed, R. P., and J. M. Reed  2004   \n",
       "2  Journal Article  Distefano, R. J., R. J. Neves, L. A. Helfrich,...  1991   \n",
       "3  Journal Article                                       Fauth, J. E.  1990   \n",
       "4  Journal Article      Gelder, S. R., L. McCurry, and D. F. McAlpine  2009   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Crayfish predation on corbicula under laborato...   \n",
       "1  Ecosystem engineering by crayfish in a headwat...   \n",
       "2  Response of the crayfish cambarus bartonii bar...   \n",
       "3  Interactive effects of predators and early lar...   \n",
       "4  Distribution and first records of branchiobdel...   \n",
       "\n",
       "                                        Journal Name Volume Issue      Pages  \\\n",
       "0                        American Midland Naturalist    105     1    181-188   \n",
       "1  Journal of the North American Benthological So...     23     2    224-236   \n",
       "2                        Canadian Journal of Zoology     69     6  1585-1591   \n",
       "3                                            Ecology     71     4  1609-1616   \n",
       "4                            Northeastern Naturalist     16     1      45-52   \n",
       "\n",
       "                                                 URL Keywords  \\\n",
       "0                                                NaN      NaN   \n",
       "1  http://www.journals.uchicago.edu/doi/citedby/1...      NaN   \n",
       "2  https://app.dimensions.ai/details/publication/...      NaN   \n",
       "3  https://app.dimensions.ai/details/publication/...      NaN   \n",
       "4  https://bioone.org/journals/northeastern-natur...      NaN   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Crayfish predation on the Asiatic clam Corbicu...   \n",
       "1  AbstractWe conducted an enclosure/exclosure ex...   \n",
       "2  Intermolt adult and juvenile Cambarus bartonii...   \n",
       "3  Ecologists need to determine the frequency and...   \n",
       "4  Preserved specimens of the crayfish Cambarus b...   \n",
       "\n",
       "                                               DOI  \\\n",
       "0                                  10.2307/2425023   \n",
       "1  10.1899/0887-3593(2004)023<0224:eebcia>2.0.co;2   \n",
       "2                                  10.1139/z91-222   \n",
       "3                                  10.2307/1938296   \n",
       "4                             10.1656/045.016.0104   \n",
       "\n",
       "                                            PDF Name  \n",
       "0  internal-pdf://1550862459/Covich-1981-Crayfish...  \n",
       "1  internal-pdf://1645103618/Creed-2004-Ecosystem...  \n",
       "2  internal-pdf://0683493655/Distefano-1991-Respo...  \n",
       "3  internal-pdf://4101458137/Fauth-1990-Interacti...  \n",
       "4  internal-pdf://2457356880/Gelder-2009-Distribu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Open a file dialog to select an Excel file\n",
    "file_path = filedialog.askopenfilename()\n",
    "\n",
    "# Create new column names because text file has no header\n",
    "col_names = [\"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Issue\", \"Pages\", \"URL\", \"Keywords\", \"Abstract\", \"DOI\", \"PDF Name\"]\n",
    "\n",
    "# Convert text file into a dataframe\n",
    "df = pd.read_csv(file_path, sep = '\\t', header = None, dtype = str, names = col_names, quotechar = '\"')\n",
    "\n",
    "# Top five rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90e4f7",
   "metadata": {},
   "source": [
    "Select the file location of the pdfs you want uploaded:\n",
    "Before running the cell below, make sure that your PDFs files are in seperate folder. After running this cell, another file dialog box will open. Select the folder where you have stored the PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d9589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Open a file dialog to select PDF file location\n",
    "pdf_folder = filedialog.askdirectory(title=\"Select a Folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf35dd6",
   "metadata": {},
   "source": [
    "## 4. Cleaning References:\n",
    "\n",
    "The following code bloack do quite a bit:\n",
    "* Finds Duplicates\n",
    "* Add Location column with 'NAS' as location\n",
    "* Add 'Species Data Entered' with 'N'\n",
    "* Add 'Impact Data Entered' with 'N'\n",
    "* Extracts keywords from PDFs and adds in scientific and commmon names\n",
    "* Removes illegal characters from abstracts\n",
    "* Removes URL if DOI if present\n",
    "* Cleans DOI column\n",
    "* Cleans PDF file names column\n",
    "* Corrects title capitalization\n",
    "* Adds hypertext to species names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd507cb",
   "metadata": {},
   "source": [
    "### FInd Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b613ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT!\n",
    "\n",
    "# Duplications \n",
    "\n",
    "#url creation\n",
    "url = 'https://nas.er.usgs.gov/queries/references/ReferenceList.aspx'\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # extract search parameters references\n",
    "    lead_author = str(row['Author']).split(',', 1)[0] + ','\n",
    "    date = row['Year']\n",
    "    title_start = ' '.join(str(row['Title']).split()[:5])\n",
    "    \n",
    "    #set queary parameters\n",
    "    search_params = {\n",
    "        \"refnum\": '',\n",
    "        \"author\": lead_author,\n",
    "        \"date\": date,\n",
    "        \"title\":title_start,\n",
    "        \"journal\": '',\n",
    "        \"publisher\": '',\n",
    "        \"vol\": '',\n",
    "        \"issue\":'',\n",
    "        \"pages\":'',\n",
    "        \"URL\":'',\n",
    "        \"key_words\":'',\n",
    "        \"type\":'' \n",
    "    }\n",
    "    \n",
    "    # call url\n",
    "    response = requests.post(url, params = search_params)\n",
    "     \n",
    "    # scrape the RefNum \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        #find specific tag using id attribute\n",
    "        desired_a_tag = soup.find(\"a\", {\"id\": \"ContentPlaceHolder1_GridView1_HyperLink1_0\"})\n",
    "        \n",
    "        if desired_a_tag:\n",
    "            refnum = desired_a_tag.get_text(strip = True)\n",
    "            results.append(refnum)\n",
    "        else:\n",
    "            refnum = 'No'\n",
    "            results.append(refnum)\n",
    "            \n",
    " # add column\n",
    "df['Duplicate'] = results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a245e",
   "metadata": {},
   "source": [
    "### Add Location, Specimen Data Entered, and Impacts Data Entered column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567b3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Add for location, specimen data, and impact data\n",
    "df['Location'] = 'NAS'\n",
    "df['Specimen Data Entered'] = 'N'\n",
    "df['Impacts Data Entered'] = 'N'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349d3d8",
   "metadata": {},
   "source": [
    "### Clean Abstract, URL, DOI, and PDF Name columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a4d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Clean DOI\n",
    "def clean_doi(text):\n",
    "    if pd.notna(text):\n",
    "        if 'doi.org' in text:\n",
    "            return text.replace('https://doi.org/', '')\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df['DOI'] = df['DOI'].apply(clean_doi)\n",
    "\n",
    "\n",
    "# Define a function to remove illegal characters\n",
    "def remove_illegal_chars(text):\n",
    "    if pd.isnull(text):  # Check if the cell is empty\n",
    "        return ''\n",
    "    # Define the pattern for illegal characters\n",
    "    illegal_chars_pattern = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]')\n",
    "    \n",
    "    # Replace illegal characters with an empty string\n",
    "    cleaned_text = illegal_chars_pattern.sub('', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df['Abstract'] = df['Abstract'].apply(remove_illegal_chars) \n",
    "    \n",
    "# URL Cleaning\n",
    "def clear_cell(row):\n",
    "    if not pd.isna(row['DOI']):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return row['URL']\n",
    "    \n",
    "df['URL'] = df.apply(clear_cell, axis = 1)\n",
    "\n",
    "# Remove bad URLs\n",
    "def remove_non_url(text):\n",
    "    if pd.notna(text):\n",
    "        if '<Go to ISI>:' in text:\n",
    "            return None\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "df['URL'] = df['URL'].apply(remove_non_url)\n",
    "\n",
    "# Clean PDF file name\n",
    "def clean_pdf_name(text):\n",
    "    if isinstance(text, str):\n",
    "        pdf_name = re.search(r'[^/]+$', text).group()\n",
    "        return pdf_name\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "df['PDF Name'] = df['PDF Name'].apply(clean_pdf_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ab0cc",
   "metadata": {},
   "source": [
    "### Add Keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3947ec45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error row 3: \n",
      "Authors: Distefano, R. J., R. J. Neves, L. A. Helfrich, and M. C. Lewis \n",
      "Year: 1991 \n",
      "Title: Response of the crayfish cambarus bartonii bartonii to acid exposure in southern appalachian streams \n",
      "PDF: Distefano-1991-Response of the crayfish cambar.pdf\n",
      "\n",
      "Error row 4: \n",
      "Authors: Fauth, J. E. \n",
      "Year: 1990 \n",
      "Title: Interactive effects of predators and early larval dynamics of the treefrog hyla chrysocelis \n",
      "PDF: Fauth-1990-Interactive effects of predators an.pdf\n",
      "\n",
      "Error row 5: \n",
      "Authors: Gelder, S. R., L. McCurry, and D. F. McAlpine \n",
      "Year: 2009 \n",
      "Title: Distribution and first records of branchiobdellida (annelida: Clitellata) from crayfishes (crustacea: Decapoda) in the maritime provinces of Canada \n",
      "PDF: Gelder-2009-Distribution and First Records of.pdf\n",
      "\n",
      "Error row 6: \n",
      "Authors: Griffith, M. B., L. T. Wolcott, and S. A. Perry \n",
      "Year: 1996 \n",
      "Title: Production of the crayfish cambarus bartonii (fabricius, 1798) (decapoda, cambaridae) in an acidic appalachian stream (USA) \n",
      "PDF: Griffith-1996-Production of the crayfish camba.pdf\n",
      "\n",
      "Error row 7: \n",
      "Authors: Hadley, K. R., A. M. Paterson, R. A. Reid, J. A. Rusak, K. M. Somers, R. Ingram, and J. P. Smol \n",
      "Year: 2015 \n",
      "Title: Altered ph and reduced calcium levels drive near extirpation of native crayfish, <i>cambarus bartonii</i>, in algonquin park, ontario, Canada \n",
      "PDF: Hadley-2015-Altered ph and reduced calcium lev.pdf\n",
      "\n",
      "Error row 8: \n",
      "Authors: Hartman, K. J., C. D. Horn, and P. M. Mazik \n",
      "Year: 2010 \n",
      "Title: Influence of elevated temperature and acid mine drainage on mortality of the crayfish cambarus bartonii \n",
      "PDF: Hartman-2010-Influence of Elevated Temperature.pdf\n",
      "\n",
      "Error row 9: \n",
      "Authors: Hartzell, S. M. \n",
      "Year: 2017 \n",
      "Title: A bilaterally partitioned colour variant of an appalachian brook crayfish (<i>cambarus bartonii bartonii</i>) from eastern pennsylvania \n",
      "PDF: Hartzell-2017-A Bilaterally Partitioned Colour.pdf\n",
      "\n",
      "Error row 10: \n",
      "Authors: Hartzell, S. M. \n",
      "Year: 2019 \n",
      "Title: Observation of a cambarus bartonii bartonii (common crayfish) overwintering in a terrestrial winter microhabitat \n",
      "PDF: Hartzell-2019-Observation of a Cambarus barton.pdf\n",
      "\n",
      "Error row 11: \n",
      "Authors: Jezerinac, R. F. \n",
      "Year: 1982 \n",
      "Title: Life-history notes and distributions of crayfishes (decapoda: Cambaridae) from the chagrin river basin, northeastern ohio) \n",
      "PDF: Jezerinac-1982-Life-history notes and distribu.pdf\n",
      "\n",
      "Error row 12: \n",
      "Authors: Jezerinac, R. F. \n",
      "Year: 1985 \n",
      "Title: Morphological variations of cambarus (cambarus) bartonii cavatus (decapoda: Cambaridae) from ohio, with a diagnosis of the ohio form \n",
      "PDF: Jezerinac-1985-Morphological variations of cam.pdf\n",
      "\n",
      "Error row 13: \n",
      "Authors: Keenan, S. W., M. L. Niemiller, and B. W. Williams \n",
      "Year: 2014 \n",
      "Title: Observations of cambarus bartonii cavatus (decapoda: Cambaridae) and ectosymbiotic branchiobdellidans (annelida: Clitellata) in cruze cave, knox county, tennessee, USA \n",
      "PDF: Keenan-2014-Observations of cambarus bartonii.pdf\n",
      "\n",
      "Error row 15: \n",
      "Authors: Neville, C. M., M. L. Cubbage, M. R. Stewart, C. J. Grant, and N. Z. Muth \n",
      "Year: 2020 \n",
      "Title: Local ecosystem uptake of stocked trout by cambarus bartonii and the relevance of prior exposure to stocking \n",
      "PDF: Neville-2020-Local Ecosystem Uptake of Stocked.pdf\n",
      "\n",
      "Error row 16: \n",
      "Authors: Schofield, K. A., C. M. Pringle, J. L. Meyer, and A. B. Sutherland \n",
      "Year: 2001 \n",
      "Title: The importance of crayfish in the breakdown of rhododendron leaf litter \n",
      "PDF: Schofield-2001-The importance of crayfish in t.pdf\n",
      "\n",
      "Error row 17: \n",
      "Authors: Seiler, S. M., and A. M. Turner \n",
      "Year: 2004 \n",
      "Title: Growth and population size of crayfish in headwater streams: Individual‐ and higher‐level consequences of acidification \n",
      "PDF: Seiler-2004-Growth and population size of cray.pdf\n",
      "\n",
      "Error row 18: \n",
      "Authors: Sherba, M., D. W. Dunham, and H. H. Harvey \n",
      "Year: 2000 \n",
      "Title: Sublethal copper toxicity and food response in the freshwater crayfish cambarus bartonii (cambaridae, decapoda, crustacea) \n",
      "PDF: Sherba-2000-Sublethal copper toxicity and food.pdf\n",
      "\n",
      "Error row 19: \n",
      "Authors: Zachary, J. L., P. S. Thomas, and A. W. Stuart \n",
      "Year: 2009 \n",
      "Title: West virginia crayfishes (decapoda: Cambaridae): Observations on distribution, natural history, and conservation \n",
      "PDF: Zachary-2009-West Virginia Crayfishes (Decapod.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Add keywords\n",
    "def keyword_find(filename):\n",
    "    \n",
    "    # Open file\n",
    "    file = fitz.open(filename)\n",
    "    \n",
    "    # Read and block only the first two pages of PDF\n",
    "    text = []\n",
    "    for i, page in enumerate(file):\n",
    "        if i > 1:\n",
    "            break\n",
    "        text += page.get_text(\"blocks\")\n",
    "    \n",
    "    # Close file\n",
    "    file.close()\n",
    "    \n",
    "    # Find block containing keywords - if no keywords in PDF, common & scientific names used\n",
    "    for block in text:\n",
    "        if block[4].lower().startswith('key-words:'):\n",
    "            pdf_keywords = block[4][10:].strip()\n",
    "            break\n",
    "        elif block[4].lower().startswith('key words:'):\n",
    "            pdf_keywords = block[4][10:].strip()\n",
    "            break\n",
    "        elif block[4].lower().startswith('keywords:'):\n",
    "            pdf_keywords = block[4][9:].strip()\n",
    "            break\n",
    "        elif block[4].lower().startswith('key-words'):\n",
    "            pdf_keywords = block[4][9:].strip()\n",
    "            break\n",
    "        elif block[4].lower().startswith('key words'):\n",
    "            pdf_keywords = block[4][9:].strip()\n",
    "            break\n",
    "        elif block[4].lower().startswith('keywords'):\n",
    "            pdf_keywords = block[4][8:].strip()\n",
    "            break\n",
    "        else:\n",
    "            pdf_keywords = '' #species_name\n",
    "        \n",
    "    # Replace intermediate characters - this list is not exhaustive\n",
    "    keywords_replace = re.sub(r'[\\n;�\\xa0·./]', ', ', pdf_keywords).replace(', ,', ',').strip(',')\n",
    "    \n",
    "    # Combine keywords with scientific and common names\n",
    "    clean_keywords = keywords_replace.split(',')\n",
    "    \n",
    "    return(clean_keywords)\n",
    "\n",
    "\n",
    "# Remove breaks created by EndNote\n",
    "df['Keywords'] = df['Keywords'].str.replace('\\r\\n', ', ')\n",
    "\n",
    "# Convert each row into a list\n",
    "df['Keywords'] = df['Keywords'].apply(lambda x: [x] if pd.notnull(x) else [])\n",
    "\n",
    "# Iterate through DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # Create file path\n",
    "        file_path = os.path.join(pdf_folder, row['PDF Name'])\n",
    "        \n",
    "        # Extract keywords - ignore if blank\n",
    "        combined_keywords = species_names + [item for item in row['Keywords'] if item] + [item for item in keyword_find(file_path) if item]\n",
    "        \n",
    "        # Remove duplicates\n",
    "        combined_keywords = list(set(combined_keywords))\n",
    "        \n",
    "        # Remove extra spaces around words\n",
    "        combined_keywords_strip = [re.sub(r'\\s+', ' ', term).strip() for term in combined_keywords]\n",
    "        \n",
    "        # Convert list to string\n",
    "        keywords = ', '.join(combined_keywords_strip)\n",
    "        \n",
    "        # Update 'Keyword' column\n",
    "        df.at[index, 'Keywords'] = keywords\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error row {index + 1}: \\nAuthors: {row['Author']} \\nYear: {row['Year']} \\nTitle: {row['Title']} \\nPDF: {row['PDF Name']}\\n\")\n",
    "        \n",
    "        # Skip to the next iteration of the loop\n",
    "        continue  # Skip to the next iteration of the loop\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867efcc2",
   "metadata": {},
   "source": [
    "### Title Correction:\n",
    "The following code uses a machine learning model to 'truecase' the journal article titles. \n",
    "\n",
    "* Be Aware!: The underlying model for this is not perfect/needs more work. There will be some errors because there is not yet a good way to account for all variation in proper nouns. If you would rather do this step yourself, you don't need to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b1e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "class TrueCaser(object):\n",
    "    def __init__(self, dist_file_path=None):\n",
    "\n",
    "        \"\"\" Initialize module with the model from Google Drive \"\"\"\n",
    "        if dist_file_path is None:\n",
    "            dist_file_path = 'models/truecaserTest.dist'\n",
    "            \n",
    "        with open(dist_file_path, \"rb\") as distributions_file:\n",
    "            pickle_dict = pickle.load(distributions_file)\n",
    "            self.uni_dist = pickle_dict[\"uni_dist\"]\n",
    "            self.backward_bi_dist = pickle_dict[\"backward_bi_dist\"]\n",
    "            self.forward_bi_dist = pickle_dict[\"forward_bi_dist\"]\n",
    "            self.trigram_dist = pickle_dict[\"trigram_dist\"]\n",
    "            self.word_casing_lookup = pickle_dict[\"word_casing_lookup\"]\n",
    "        self.detknzr = TreebankWordDetokenizer()\n",
    "\n",
    "    def get_score(self, prev_token, possible_token, next_token):\n",
    "        pseudo_count = 5.0\n",
    "\n",
    "        # Get Unigram Score\n",
    "        numerator = self.uni_dist[possible_token] + pseudo_count\n",
    "        denominator = 0\n",
    "        for alternativeToken in self.word_casing_lookup[\n",
    "                possible_token.lower()]:\n",
    "            denominator += self.uni_dist[alternativeToken] + pseudo_count\n",
    "\n",
    "        unigram_score = numerator / denominator\n",
    "\n",
    "        # Get Backward Score\n",
    "        bigram_backward_score = 1\n",
    "        if prev_token is not None:\n",
    "            numerator = (\n",
    "                self.backward_bi_dist[prev_token + \"_\" + possible_token] +\n",
    "                pseudo_count)\n",
    "            denominator = 0\n",
    "            for alternativeToken in self.word_casing_lookup[\n",
    "                    possible_token.lower()]:\n",
    "                denominator += (self.backward_bi_dist[prev_token + \"_\" +\n",
    "                                                      alternativeToken] +\n",
    "                                pseudo_count)\n",
    "\n",
    "            bigram_backward_score = numerator / denominator\n",
    "\n",
    "        # Get Forward Score\n",
    "        bigram_forward_score = 1\n",
    "        if next_token is not None:\n",
    "            next_token = next_token.lower()  # Ensure it is lower case\n",
    "            numerator = (\n",
    "                self.forward_bi_dist[possible_token + \"_\" + next_token] +\n",
    "                pseudo_count)\n",
    "            denominator = 0\n",
    "            for alternativeToken in self.word_casing_lookup[\n",
    "                    possible_token.lower()]:\n",
    "                denominator += (\n",
    "                    self.forward_bi_dist[alternativeToken + \"_\" + next_token] +\n",
    "                    pseudo_count)\n",
    "\n",
    "            bigram_forward_score = numerator / denominator\n",
    "\n",
    "        # Get Trigram Score\n",
    "        trigram_score = 1\n",
    "        if prev_token is not None and next_token is not None:\n",
    "            next_token = next_token.lower()  # Ensure it is lower case\n",
    "            numerator = (self.trigram_dist[prev_token + \"_\" + possible_token +\n",
    "                                           \"_\" + next_token] + pseudo_count)\n",
    "            denominator = 0\n",
    "            for alternativeToken in self.word_casing_lookup[\n",
    "                    possible_token.lower()]:\n",
    "                denominator += (\n",
    "                    self.trigram_dist[prev_token + \"_\" + alternativeToken +\n",
    "                                      \"_\" + next_token] + pseudo_count)\n",
    "\n",
    "            trigram_score = numerator / denominator\n",
    "\n",
    "        result = (math.log(unigram_score) + math.log(bigram_backward_score) +\n",
    "                  math.log(bigram_forward_score) + math.log(trigram_score))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def first_token_case(self, raw):\n",
    "        return raw.capitalize()\n",
    "\n",
    "    def get_true_case(self, sentence, out_of_vocabulary_token_option=\"title\"):\n",
    "        \"\"\" Wrapper function for handling untokenized input.\n",
    "\n",
    "        @param sentence: a sentence string to be tokenized\n",
    "        @param outOfVocabularyTokenOption:\n",
    "            title: Returns out of vocabulary (OOV) tokens in 'title' format\n",
    "            lower: Returns OOV tokens in lower case\n",
    "            as-is: Returns OOV tokens as is\n",
    "\n",
    "        Returns (str): detokenized, truecased version of input sentence\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens_true_case = self.get_true_case_from_tokens(tokens, out_of_vocabulary_token_option)\n",
    "        return self.detknzr.detokenize(tokens_true_case)\n",
    "\n",
    "    def get_true_case_from_tokens(self, tokens, out_of_vocabulary_token_option=\"title\"):\n",
    "        \"\"\" Returns the true case for the passed tokens.\n",
    "\n",
    "        @param tokens: List of tokens in a single sentence\n",
    "        @param pretokenised: set to true if input is alreay tokenised (e.g. string with whitespace between tokens)\n",
    "        @param outOfVocabularyTokenOption:\n",
    "            title: Returns out of vocabulary (OOV) tokens in 'title' format\n",
    "            lower: Returns OOV tokens in lower case\n",
    "            as-is: Returns OOV tokens as is\n",
    "\n",
    "        Returns (list[str]): truecased version of input list\n",
    "        of tokens\n",
    "        \"\"\"\n",
    "        tokens_true_case = []\n",
    "        for token_idx, token in enumerate(tokens):\n",
    "\n",
    "            if token in string.punctuation or token.isdigit():\n",
    "                tokens_true_case.append(token)\n",
    "            else:\n",
    "                token = token.lower()\n",
    "                if token in self.word_casing_lookup:\n",
    "                    if len(self.word_casing_lookup[token]) == 1:\n",
    "                        tokens_true_case.append(\n",
    "                            list(self.word_casing_lookup[token])[0])\n",
    "                    else:\n",
    "                        prev_token = (tokens_true_case[token_idx - 1]\n",
    "                                      if token_idx > 0 else None)\n",
    "                        next_token = (tokens[token_idx + 1]\n",
    "                                      if token_idx < len(tokens) - 1 else None)\n",
    "\n",
    "                        best_token = None\n",
    "                        highest_score = float(\"-inf\")\n",
    "\n",
    "                        for possible_token in self.word_casing_lookup[token]:\n",
    "                            score = self.get_score(prev_token, possible_token,\n",
    "                                                   next_token)\n",
    "\n",
    "                            if score > highest_score:\n",
    "                                best_token = possible_token\n",
    "                                highest_score = score\n",
    "\n",
    "                        tokens_true_case.append(best_token)\n",
    "\n",
    "                    if token_idx == 0:\n",
    "                        tokens_true_case[0] = self.first_token_case(\n",
    "                            tokens_true_case[0])\n",
    "\n",
    "                else:  # Token out of vocabulary\n",
    "                    if out_of_vocabulary_token_option == \"title\":\n",
    "                        tokens_true_case.append(token.title())\n",
    "                    elif out_of_vocabulary_token_option == \"capitalize\":\n",
    "                        tokens_true_case.append(token.capitalize())\n",
    "                    elif out_of_vocabulary_token_option == \"lower\":\n",
    "                        tokens_true_case.append(token.lower())\n",
    "                    else:\n",
    "                        tokens_true_case.append(token)\n",
    "\n",
    "        return tokens_true_case\n",
    "    \n",
    "# Upload Truecaser model\n",
    "caser = TrueCaser('models/truecaserTest.dist')\n",
    "\n",
    "# Function to apply get_true_case to a sentence\n",
    "def apply_true_case(sentence):\n",
    "    return caser.get_true_case(sentence, \"lower\")\n",
    "\n",
    "\n",
    "# Apply the function to the 'sentences' column\n",
    "df['Title'] = df['Title'].apply(apply_true_case)\n",
    "\n",
    "# Function to capitalize the first letter of the first word\n",
    "def capitalize_first_word(text):\n",
    "    return text[:1].capitalize() + text[1:]\n",
    "\n",
    "# Apply the function to the specified column\n",
    "df['Title'] = df['Title'].apply(capitalize_first_word)\n",
    "\n",
    "\n",
    "# Function to remove <i> and </i> tags from the text\n",
    "def remove_italic_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Apply the function to the 'text_column' and store the result in a new column\n",
    "df['Title'] = df['Title'].apply(remove_italic_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906d86c",
   "metadata": {},
   "source": [
    "### HTML Hypertext\n",
    "This will add hypertext (&lt;em&gt;) around scientific names to italicize them in the titles and abstracts.\n",
    "\n",
    "* Adding in hypertext can make it difficult to read the titles. If you want to enter this yourself, you do not need to run the following code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b95e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Upload text file with species names with subspecies list\n",
    "species_names_with_subspecies = pd.read_excel('textFiles/subspecies.xlsx', header = None)\n",
    "species_names_with_subspecies = species_names_with_subspecies.iloc[:, 0].to_list()\n",
    "\n",
    "# Upload text file with species names list\n",
    "species_names = pd.read_excel('textFiles/scientific_names.xlsx', header = None)\n",
    "species_names = species_names.iloc[:, 0].to_list()\n",
    "\n",
    "# Upload text file with genus list\n",
    "genus = pd.read_excel('textFiles/genus.xlsx', header = None)\n",
    "genus = genus.iloc[:, 0].to_list()\n",
    "\n",
    "# Forumala to surround text with <em>\n",
    "def surround_with_em(text, words_to_surround):\n",
    "    if isinstance(text, str):\n",
    "        for word in words_to_surround:\n",
    "            pattern = r'(?<!<em>)\\b' + re.escape(word.strip()) + r'\\b(?!<\\/em>)'\n",
    "            replacement = r'<em>\\g<0></em>'\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the replacement to species names with subspecies\n",
    "df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, species_names_with_subspecies))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, species_names_with_subspecies))\n",
    "\n",
    "# Apply the replacement to species names\n",
    "df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, species_names))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, species_names))\n",
    "\n",
    "# Apply the replacement to genus\n",
    "df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, genus))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, genus))\n",
    "\n",
    "\n",
    "# Formula to abbreviate scientific names\n",
    "def abbreviate_scientific_name(scientific_names):\n",
    "    \n",
    "    abbreviated_names = []\n",
    "        \n",
    "    for name in scientific_names:\n",
    "        words = name.split()\n",
    "        genus = words[0][0] + \".\"\n",
    "        species = \" \".join(words[1:])\n",
    "        abbreviated_names.append(genus + \" \" + species)\n",
    "        \n",
    "    return abbreviated_names\n",
    "\n",
    "abbreviated_names = abbreviate_scientific_name(species_names)\n",
    "\n",
    "# Apply the replacement to abbreviated scientific names\n",
    "df['Title'] = df['Title'].apply(lambda x: surround_with_em(x, abbreviated_names))\n",
    "df['Abstract'] = df['Abstract'].apply(lambda x: surround_with_em(x, abbreviated_names))\n",
    "\n",
    "# Reorder columns\n",
    "new_column_order = [\"Duplicate\", \"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Issue\", \"Pages\", \"URL\", \"Location\", \"Specimen Data Entered\", \"Impacts Data Entered\", \"Keywords\", \"Abstract\", \"DOI\", \"PDF Name\"]\n",
    "df = df[new_column_order]\n",
    "\n",
    "def highlight(x):\n",
    "    # Define columns to highlight for missing values and non-PDF file names\n",
    "    columns_to_highlight_na = [\"Type\", \"Author\", \"Year\", \"Title\", \"Journal Name\", \"Volume\", \"Pages\", \"Location\", \"Specimen Data Entered\", \"Impacts Data Entered\", \"PDF Name\"]\n",
    "    column_to_check_pdf = \"PDF Name\"\n",
    "    \n",
    "    # Create an empty DataFrame with the same index and columns as the input DataFrame\n",
    "    styled_df = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "    \n",
    "    # Highlight missing values\n",
    "    for col in columns_to_highlight_na:\n",
    "        mask_na = pd.isna(x[col])\n",
    "        styled_df.loc[mask_na, col] = f'background-color: #FFFF00;'\n",
    "    \n",
    "    # Handle NaN values in \"PDF Name\" column before applying bitwise NOT\n",
    "    mask_pdf_valid = ~x[column_to_check_pdf].astype(str).str.endswith('.pdf', na=False)\n",
    "    styled_df.loc[mask_pdf_valid, column_to_check_pdf] = 'background-color: #FFFF00;'\n",
    "    \n",
    "    # Return the styled DataFrame\n",
    "    return styled_df\n",
    "\n",
    "# Apply styling to the original DataFrame\n",
    "styled_df = df.style.apply(highlight, axis=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02a0c9",
   "metadata": {},
   "source": [
    "## 5. Export file\n",
    "The following cell will format reference dataframe and export as an Excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50c42737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export successful!\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "# Export Excel\n",
    "try:\n",
    "    file_path = filedialog.asksaveasfilename(defaultextension=\".xlsx\", filetypes=[(\"Excel files\", \"*.xlsx\"), (\"All files\", \"*.*\")])\n",
    "    styled_df.to_excel(file_path, engine='openpyxl', index=False)\n",
    "    print(\"Export successful!\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: Illegal characters found in the data. Export aborted.\")\n",
    "except Exception as e:\n",
    "    print(\"An unexpected error occurred:\", e)\n",
    "\n",
    "# Open Excel sheet if export was successful\n",
    "if \"file_path\" in locals() and os.path.exists(file_path):\n",
    "    os.startfile(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54bc5a",
   "metadata": {},
   "source": [
    "## 6. Final Edits:\n",
    "There is still some work that needs to be done before bulk uploading these references. The following is a checklist to follow\n",
    "\n",
    "1. There are certain column that can not be blank. Cells that can not be blank are also highlighted in yellow. Fix those cells.\n",
    "2. Check the format of reference columns in case EndNote missed them, particularly Type, Year, Journal Name, Pages, PDF Name. For example, make sure the Year column only has years.\n",
    "3. Check Reference Numbers in the Duplicate column to ensure they are a duplicate. Remove duplicate rows from the sheet.\n",
    "4. Double-check proper capitalization in titles - as stated above the model is not perfect.\n",
    "5. Add hypertext (&lt;em&gt;) around any extra scientific names in titles.\n",
    "6. Add subtext (&lt;sub&gt;) and supertext (&lt;sup&gt;) hypertext around numbers part of chemical formulas or units. This can also be done in NAs reference editor.\n",
    "7. Check Issue columns: its not a required field but sometimes EndNote puts in strange formats that need corrected.\n",
    "8. Check any remaining URLs: NAS requires website URL to link directly to website of journal  with PDF/html version of journal article if available (directly to website of journal). NAS does not want URLs that lead to indexing services (ProQuest, Web of Knowledge, EBSCOHost). \n",
    "9. Double check blank DOI cells: On rare occassions PDF has DOI but EndNote does report it.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "referenceEditor",
   "language": "python",
   "name": "referenceeditor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
